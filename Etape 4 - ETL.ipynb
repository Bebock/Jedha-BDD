{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import des librairies utilisées \n",
    "\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import io\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.sql import text\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etape 1 : Export des données sous S3\n",
    "\n",
    "On va stocker toutes nos données dans un fichier .csv sur un datalake (S3).\n",
    "\n",
    "Un datalake est globalement un espace de stockage de données brutes (données structurées ou non structurées) qui peuvent provenir de sources différentes. Les datalakes sont régis par des règles ELT (Extract Load Transform). C'est à dire que les données sont stockées avant d'éventuelles transformations pour usage (dashboard, analyses...). Le datalake est orienté pour la data science. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation des données \n",
    "\n",
    "table_hotels = pd.read_csv('PATH/table_hotel_booking.csv')\n",
    "liste_villes = pd.read_csv('PATH/table_villes_coord.csv')\n",
    "data_meteo = pd.read_csv('PATH/data_meteo.csv')\n",
    "\n",
    "# Traitement des coordonées GPS des hotels\n",
    "\n",
    "table_hotels['H_lat'] = 0\n",
    "table_hotels['H_lon'] = 0\n",
    "\n",
    "for i in range(len(table_hotels)) : \n",
    "    table_hotels['H_lat'][i] = float(table_hotels['latlong'].iloc(axis = 0)[i][table_hotels['latlong'].iloc(axis = 0)[i].index(\"['\")+ 2: table_hotels['latlong'].iloc(axis = 0)[i].index(\",\")])\n",
    "    table_hotels['H_lon'][i] = float(table_hotels['latlong'].iloc(axis = 0)[i][table_hotels['latlong'].iloc(axis = 0)[i].index(\",\") + 1: table_hotels['latlong'].iloc(axis = 0)[i].index(\"']\")])\n",
    "\n",
    "# Merging des 3 données en 1 seule \n",
    "\n",
    "data = pd.merge(liste_villes, table_hotels, on = 'city')\n",
    "data = pd.merge(data, data_meteo, on = 'city')\n",
    "\n",
    "# Stockage des données dans un fichier .csv\n",
    "\n",
    "csv = data.to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connexion à AWS\n",
    "\n",
    "session = boto3.Session(aws_access_key_id = \"YOUR_ACCESS_KEY\", # Rempacer par votre key_id\n",
    "                        aws_secret_access_key = \"YOUR_SECRET_ACCESS_KEY\") # Rempacer par votre mot de passe\n",
    "\n",
    "s3 = session.resource(\"s3\")\n",
    "\n",
    "# Création du Bucket\n",
    "\n",
    "bucket =s3.create_bucket(Bucket = \"citiesandhotels2022\") # Le nom doit être unique - A updater\n",
    "\n",
    "# Import du CSV dans le bucket créé\n",
    "\n",
    "put_object = bucket.put_object(Key = \"villes_et_hotels.csv\", Body = csv)\n",
    "\n",
    "# Verification du contenu du bucket\n",
    "\n",
    "all_files = [obj.key for obj in bucket.objects.all()]\n",
    "all_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etape 2 : Extract, Transform and Load (ETL)\n",
    "\n",
    "On va ensuite pouvoir passer du datalake au data warehouse.\n",
    "\n",
    "Un entrepôt de données rassemble des données pré-traitées / nettoyées et les stocke de manière structurée. De fait, les entrepôts sont régis par des règles ETL (Extract Transform Load). C'est à dire que les données sont transformées avant le stockage. Les entrepôts sont orientés pour les business decisions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va donc créer une base de données SQL (structurée) à l'aide de AWS RDS dans laquelle on va venir stocker les données. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récupérer l'objet S3 \n",
    "\n",
    "obj = s3.get_object(\n",
    "    Bucket = 'citiesandhotels2022',\n",
    "    Key = 'villes_et_hotels.csv')\n",
    "    \n",
    "data = pd.read_csv(obj['Body'])\n",
    "\n",
    "# Connexion à AWS : Remplacer USERNAME, PASSWORD et HOSTNAME par vos informations de connexion\n",
    "\n",
    "engine = create_engine(\"postgresql+psycopg2://USERNAME:PASSWORD@HOSTNAME/postgres\", echo = True)\n",
    "conn = engine.connect()\n",
    "\n",
    "# Passer la base de données à une base sql\n",
    "\n",
    "data.to_sql(\"villes_et_hotels\", con = engine)\n",
    "\n",
    "# Lecture de la base SQL\n",
    "\n",
    "pd.read_sql(text(\" SELECT * FROM villes_et_hotels \"), con = engine)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3350b0464cf3626baf73e084855ddfc8096d35b6b5375449976ef29efd62d433"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
